#!/usr/bin/python

""" Infer stellar parameters from spectra """

from __future__ import division, print_function

__author__ = "Andy Casey <arc@ast.cam.ac.uk>"

import matplotlib
matplotlib.use("Agg")

import argparse
import logging
import os
import cPickle as pickle

from json import dumps as json_dumps
from time import time

import numpy as np
import pyfits

import scope

def main():

	parser = argparse.ArgumentParser(description="infer stellar parameters from spectra")
	parser.add_argument("model", type=str, help="YAML- or JSON-style model filename")
	parser.add_argument("spectra", nargs="+", help="filenames of spectra to analyse")
	parser.add_argument("-o", "--output-dir", dest="output_dir", nargs="?", help="directory for output files",
		type=str, default=os.getcwd())
	parser.add_argument("-p", "--filename-prefix", dest="filename_prefix", default="scope")
	parser.add_argument("-s", "--skip", dest="skip", action="store", type=int, default=0)
	parser.add_argument("--no-plots", dest="plotting", action="store_false", default=True)
	parser.add_argument("-v", "--verbose", dest="verbose", action="store_true", default=False)
	parser.add_argument("--debug", dest="debug", action="store_true", default=False)

	args = parser.parse_args()

	if not os.path.exists(args.model):
		raise IOError("model filename does not exist")

	all_spectra = [scope.Spectrum.load(filename) for filename in args.spectra]

	if args.plotting:
		# Import plotting dependencies 
		import matplotlib as mpl
		mpl.use("Agg")

		import matplotlib.pyplot as plt
		import triangle

	# Are there multiple spectra in each aperture?
	if isinstance(all_spectra[0], list):
		# If so, they should all have the same length (e.g. same number of stars)
		if len(set(map(len, all_spectra))) > 1:
			raise IOError("filenames contain different number of spectra")

		# OK, they have the same length. They are probably apertures of the same
		# stars. Let's join them properly
		sorted_spectra = []
		num_stars, num_apertures = len(all_spectra[0]), len(all_spectra)
		for i in xrange(num_stars):
			sorted_spectra.append([all_spectra[j][i] for j in xrange(num_apertures)])

		all_spectra = sorted_spectra

	# Load the model
	model = scope.Model(args.model)
	logging.info("Model information: {0}".format(model))

	# Define headers that we want in the results filename 
	default_headers = ("RA", "DEC", "COMMENT", "ELAPSED", "FIBRE_NUM", "LAT_OBS", "LONG_OBS",
		"MAGNITUDE","NAME", "OBJECT", "RO_GAIN", "RO_NOISE", "UTDATE", "UTEND", "UTSTART", )
	default_metadata = {
		"model": model.hash, 
		"filenames": ", ".join(args.spectra),
		"scope_version": scope.__version__,
		"walkers": model.configuration["solver"]["walkers"],
		"burn": model.configuration["solver"]["burn"],
		"sample": model.configuration["solver"]["sample"],
		"final_mean_acceptance_fraction": 0
	}

	# For each spectra, analyse
	for i, spectra in enumerate(all_spectra):
		# Create metadata and put header information in
		if args.skip > i:
			logging.info("Skipping star #{0}".format(i))
			continue

		if not isinstance(spectra, (list, tuple)):
			spectra = [spectra]
			default_metadata["filenames"] = args.spectra[i]

		metadata = {}
		header_columns = []
		bluest_spectrum = spectra[0] if isinstance(spectra, list) else spectra
		for header in default_headers:
			if header not in bluest_spectrum.headers: continue
			header_columns.append(header)

			metadata[header] = bluest_spectrum.headers[header]

		# Set defaults for metadata
		metadata.update({"run_id": i})
		metadata.update(default_metadata)

		try:
			t_init = time()
			posteriors, sampler, mean_acceptance_fractions = scope.solve(spectra, model)

			
		except:
			logging.exception("Failed to analyse #{0}:".format(i))
			if args.debug: raise

			try: all_results
			except NameError:
				# Cannot add to results because we don't know all the model dimensions. We must continue
				# until we find one result that works
				continue
			else:
				row = np.core.records.fromrecords([[metadata.get(key, np.nan) for key in metadata_columns]],
					dtype=all_results.dtype)
				all_results = np.append(all_results, row)

		else:
			# Save information related to the analysis
			metadata.update({
				"time_elapsed": int(time() - t_init),
				"final_mean_acceptance_fraction": mean_acceptance_fractions[-1],
				"last_100_mean_acceptance_fraction_sigma": np.std(mean_acceptance_fractions[-100:])
			})

			# Update results with the posteriors
			logging.info("Posteriors:")
			for dimension, (posterior_value, pos_uncertainty, neg_uncertainty) in posteriors.iteritems():
				logging.info("\t{0:15s}: {1:.2f} (+{2:.2f}, -{3:.2f})".format(dimension, posterior_value,
					pos_uncertainty, neg_uncertainty))

				metadata.update({
					dimension: posterior_value,
					"u_maxabs_{0}".format(dimension): np.abs([neg_uncertainty, pos_uncertainty]).max(),
					"u_pos_{0}".format(dimension): pos_uncertainty,
					"u_neg_{0}".format(dimension): neg_uncertainty
				})

			# Save the sampler chain
			blobs = np.array(sampler.blobs)
			blobs = blobs.reshape((-1, 1 + len(model.dimensions)))

			# Append an sample and step number
			walkers = model.configuration["solver"]["walkers"]
			chain = np.core.records.fromarrays(
				np.vstack([
					np.arange(1, 1 + len(blobs)),
					np.floor(np.arange(1, 1 + len(blobs)/walkers, 1.0/walkers)),
					blobs.T
				]),
				names=["Iteration", "Sample"] + model.dimensions + ["log(likelihood)"],
				formats=["i4", "i4"] + ["f8"] * (1 + len(model.dimensions)))

			filename_prefix = os.path.join(args.output_dir, "{0}-{1}".format(args.filename_prefix, i))

			# Save the chain
			primary_hdu = pyfits.PrimaryHDU()
			table_hdu = pyfits.BinTableHDU(chain)
			hdulist = pyfits.HDUList([primary_hdu, table_hdu])
			hdulist.writeto(filename_prefix + "-chain.fits", clobber=True)

			# Save the posteriors and mean acceptance fractions
			with open(filename_prefix + ".results", "wb") as fp:
				pickle.dump((posteriors, mean_acceptance_fractions), fp)

			try: metadata_columns
			except NameError:
				# Headers first
				metadata_columns = [] + header_columns

				# Dimensions (w/ uncertainties)
				for column in model.grid_points.dtype.names:
					metadata_columns.append(column)
					for prefix in ("u_maxabs_", "u_pos_", "u_neg_"):
						metadata_columns.append(prefix + column)

				# Nusiance parameters (w/ uncertainties)
				nusiance_columns = scope.utils.unique_preserved_list(list(model.grid_points.dtype.names) + model.dimensions)[len(model.grid_points.dtype.names):]
				for column in nusiance_columns:
					metadata_columns.append(column)
					for prefix in ("u_maxabs_", "u_pos_", "u_neg_"):
						metadata_columns.append(prefix + column)

				# Time elapsed, model
				metadata_columns.extend(["time_elapsed", "model", "filenames"])
				metadata_formats = [("f8", "|S1024")[isinstance(metadata[key], str)] for key in metadata_columns]

				all_results = np.core.records.fromrecords([[metadata[key] for key in metadata_columns]],
					names=metadata_columns, formats=metadata_formats)

			else:
				row = np.core.records.fromrecords([[metadata.get(key, np.nan) for key in metadata_columns]],
					dtype=all_results.dtype)
				all_results = np.append(all_results, row)

			# Write results to filename	
			primary_hdu = pyfits.PrimaryHDU()
			table_hdu = pyfits.BinTableHDU(all_results)
			hdulist = pyfits.HDUList([primary_hdu, table_hdu])
			hdulist.writeto(os.path.join(args.output_dir, "{0}-results.fits".format(args.filename_prefix)), clobber=True)

			# Close sampler pool
			if model.configuration["solver"].get("walkers", 1) > 1:
				sampler.pool.close()
				sampler.pool.join()

			# Save sampler state
			with open(filename_prefix + ".state", "wb") as fp:
				pickle.dump([sampler.chain[:, -1, :], sampler.lnprobability[:, -1], sampler.random_state], fp, -1)

			# Save the maximum likelihood spectra
			me_index = np.argmax(blobs[:, -1])
			me_parameters = dict(zip(model.dimensions, blobs[me_index, :-1]))
			me_observed_spectra = model.observed_spectra(spectra, **me_parameters)
			me_model_spectra = model.model_spectra(observations=me_observed_spectra, **me_parameters)
			
			for aperture, modelled_aperture, observed_aperture in zip(model._mapped_apertures, me_model_spectra, me_observed_spectra):
				modelled_aperture.save(filename_prefix + "-ml-mod-{0}.fits".format(aperture))
				modelled_aperture.save(filename_prefix + "-ml-obs-{0}.fits".format(aperture))

			# Plot results
			if args.plotting:

				# Plot the mean acceptance fractions
				fig = plt.figure()
				ax = fig.add_subplot(111)
				ax.plot(mean_acceptance_fractions, "k", lw=2)
				ax.axvline(x=model.configuration["solver"]["burn"], c="#666666", zorder=-1, lw=1)
				ax.set_xlabel("Iterations")
				ax.set_ylabel("$\langle{}f_a\\rangle$")
				fig.savefig(filename_prefix + "-acceptance.jpg")

				# Make a corner plot
				ndim = sampler.chain.shape[-1]
				#fig = triangle.corner(sampler.chain.reshape((-1, ndim)),
				#	labels=scope.utils.latexify(model.dimensions), extents=[0.99]*len(model.dimensions))
				#fig.savefig(filename_prefix + "-corner.pdf")

				# Make another corner plot with just the parameters of interest
				indices = np.array([model.dimensions.index(dimension) for dimension in model.grid_points.dtype.names])
				fig = triangle.corner(sampler.chain.reshape((-1, ndim))[int(model.configuration["solver"]["walkers"] * model.configuration["solver"]["burn"]):, indices],
					labels=scope.utils.latexify(model.grid_points.dtype.names), extents=[0.99]*len(model.grid_points.dtype.names))
				fig.savefig(filename_prefix + "-corner.jpg")

				# Get the most probable spectra
				me_index = np.argmax(blobs[:, -1])
				me_parameters = dict(zip(model.dimensions, blobs[me_index, :-1]))

				fig = plt.figure()
				for i, (aperture, modelled_aperture, observed_aperture) in enumerate(zip(model._mapped_apertures, me_model_spectra, me_observed_spectra)):

					ax = fig.add_subplot(len(spectra), 1, i+1)
					ax.plot(modelled_aperture.disp, modelled_aperture.flux, 'b')
					ax.plot(observed_aperture.disp, observed_aperture.flux, 'k')

					# Get the full boundaries
					if "masks" in model.configuration and aperture in model.configuration["masks"]:
						full_extent = \
							(np.min(model.configuration["masks"][aperture]), np.max(model.configuration["masks"][aperture]))

						ax.set_xlim(full_extent)

					ax.set_ylim(0.0, 1.1)

				fig.savefig(filename_prefix + "-ml-spectra.jpg")

				# Get the 50th quantile parameters
				mid_parameters = dict(zip(posteriors.keys(), [posteriors[k][0] for k in posteriors.keys()]))
				
				mid_observed_spectra = model.observed_spectra(spectra, **mid_parameters)
				mid_modelled_spectra = model.model_spectra(observations=mid_observed_spectra, **mid_parameters)

				fig = plt.figure()
				for i, (aperture, modelled_aperture, observed_aperture) in enumerate(zip(model._mapped_apertures, mid_modelled_spectra, mid_observed_spectra)):

					ax = fig.add_subplot(len(spectra), 1, i+1)

					ax.plot(modelled_aperture.disp, modelled_aperture.flux, 'b')
					ax.plot(observed_aperture.disp, observed_aperture.flux, 'k')

					# Get the full boundaries
					if "masks" in model.configuration and aperture in model.configuration["masks"]:
						full_extent = \
							(np.min(model.configuration["masks"][aperture]), np.max(model.configuration["masks"][aperture]))

						ax.set_xlim(full_extent)

					ax.set_ylim(0.0, 1.1)

				fig.savefig(filename_prefix + "-pp-spectra.jpg")

				# Closing the figures isn't enough; matplotlib leaks memory
				plt.close("all")

			# Delete some things
			del sampler, chain, primary_hdu, table_hdu, hdulist


if __name__ == "__main__":
	main()
