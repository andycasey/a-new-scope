#!/usr/bin/python

""" Infer stellar parameters from spectra """

from __future__ import division, print_function

__author__ = "Andy Casey <arc@ast.cam.ac.uk>"

import matplotlib
matplotlib.use("Agg")

import argparse
import logging
import os
import cPickle as pickle

from json import dumps as json_dumps
from time import time

import numpy as np
import pyfits

import scope

def main():

	parser = argparse.ArgumentParser(description="infer stellar parameters from spectra")
	parser.add_argument("model", type=str, help="YAML- or JSON-style model filename")
	parser.add_argument("spectra", nargs="+", help="filenames of spectra to analyse")
	parser.add_argument("-o", "--output-dir", dest="output_dir", nargs="?", help="directory for output files",
		type=str, default=os.getcwd())
	parser.add_argument("-p", "--filename-prefix", dest="filename_prefix", default="scope")
	parser.add_argument("-mb", "--multi-beam", action="store_true", default=False)
	parser.add_argument("-n", "--number-to-solve", dest="number_to_solve", default="all")
	parser.add_argument("-s", "--skip", dest="skip", action="store", type=int, default=0)
	parser.add_argument("--no-plots", dest="plotting", action="store_false", default=True)
	parser.add_argument("-v", "--verbose", dest="verbose", action="store_true", default=False)
	parser.add_argument("--debug", dest="debug", action="store_true", default=False)
	parser.add_argument("-pf", "--plot-format", dest="plot_format", action="store", type=str, default="jpg")
	args = parser.parse_args()

	if not os.path.exists(args.model):
		raise IOError("model filename {0} does not exist".format(args.model))

	available = ("pdf", "jpg", "png", "eps")
	if args.plotting and args.plot_format.lower() not in available:
		raise ValueError("plotting format '{0}' not available. Options: {1}".format(args.plot_format.lower(),
			", ".join(available)))

	# Initialise logging
	logger = logging.getLogger("scope")
	logger.setLevel(logging.DEBUG if args.debug else logging.INFO)

	all_spectra = [scope.Spectrum.load(filename) for filename in args.spectra]

	if args.plotting:
		# Import plotting dependencies 
		import matplotlib as mpl
		mpl.use("Agg")

		import matplotlib.pyplot as plt
		import triangle

	# Are there multiple spectra in each aperture?
	if args.multi_beam:
		# If so, they should all have the same length (e.g. same number of stars)
		if len(set(map(len, all_spectra))) > 1:
			raise IOError("filenames contain different number of spectra")

		# OK, they have the same length. They are probably apertures of the same
		# stars. Let's join them properly
		sorted_spectra = []
		num_stars, num_apertures = len(all_spectra[0]), len(all_spectra)
		for i in xrange(num_stars):
			sorted_spectra.append([all_spectra[j][i] for j in xrange(num_apertures)])

		all_spectra = sorted_spectra
	else:
		all_spectra = sum(all_spectra, [])

	# Load the model
	model = scope.Model(args.model)
	logger.info("Model information: {0}".format(model))

	# Define headers that we want in the results filename 
	default_headers = ("RA", "DEC", "COMMENT", "ELAPSED", "FIBRE_NUM", "LAT_OBS", "LONG_OBS",
		"MAGNITUDE","NAME", "OBJECT", "RO_GAIN", "RO_NOISE", "UTDATE", "UTEND", "UTSTART", )
	default_metadata = {
		"model": model.hash, 
		"input_filenames": ", ".join(args.spectra),
		"scope_version": scope.__version__,
		"walkers": model.configuration["solver"]["walkers"],
		"burn": model.configuration["solver"]["burn"],
		"sample": model.configuration["solver"]["max_sample"] - model.configuration["solver"]["burn"],
		"final_mean_acceptance_fraction": 0
	}

	all_results_filename = os.path.join(args.output_dir, "{0}-results.fits".format(args.filename_prefix))
	
	# For each spectra, analyse
	for i, spectra in enumerate(all_spectra):
		logger.info("Starting on star #{0}".format(i + 1))

		# Force spectra as a list
		if not isinstance(spectra, (list, tuple)):
			spectra = [spectra]

		# Create metadata and put header information in
		if args.skip > i:
			logger.info("Skipping star #{0}".format(i))
			continue

		if isinstance(args.number_to_solve, (int, float)) and \
		args.number_to_solve == (i + 1 + args.skip):
			logger.info("We have analysed {0} spectra. Exiting..".format(args.number_to_solve))
			break

		metadata = {}
		header_columns = []
		bluest_spectrum = spectra[0] if isinstance(spectra, list) else spectra
		for header in default_headers:
			if header not in bluest_spectrum.headers: continue
			header_columns.append(header)

			metadata[header] = bluest_spectrum.headers[header]

		# Set defaults for metadata
		metadata.update({"run_id": i})
		metadata.update(default_metadata)

		try:
			t_init = time()
			posteriors, sampler, mean_acceptance_fractions = scope.solve(spectra, model)

			
		except:
			logger.exception("Failed to analyse #{0}:".format(i))
			if args.debug: raise

			try: all_results
			except NameError:
				# Cannot add to results because we don't know all the model dimensions. We must continue
				# until we find one result that works
				continue
			else:
				row = np.core.records.fromrecords([[metadata.get(key, np.nan) for key in metadata_columns]],
					dtype=all_results.dtype)
				all_results = np.append(all_results, row)

		else:
			
			# Set some filename variables
			filename_prefix = os.path.join(args.output_dir, "{0}-{1}".format(args.filename_prefix, i))
			state_filename = "{0}.state".format(filename_prefix)
			pp_observed_spectra_filenames = ["{0}-pp-obs-{1}.fits".format(filename_prefix, aperture) \
				for aperture in model._mapped_apertures]
			pp_modelled_spectra_filenames = ["{0}-pp-mod-{1}.fits".format(filename_prefix, aperture) \
				for aperture in model._mapped_apertures]

			if args.plotting:
				chain_filename = filename_prefix + "-chain.fits"
				acceptance_plot_filename = "{0}-acceptance.{1}".format(filename_prefix, args.plot_format)
				corner_plot_filename = "{0}-corner.{1}".format(filename_prefix, args.plot_format)
				pp_spectra_plot_filename = "{0}-ml-spectra.{1}".format(filename_prefix, args.plot_format)

			else:
				chain_filename, acceptance_plot_filename, corner_plot_filename, ml_spectra_plot_filename = [""]*4

			# Save information related to the analysis
			metadata.update({
				"sample": len(mean_acceptance_fractions) - metadata["burn"],
				"state_filename": state_filename,
				"chain_filename": chain_filename,
				"acceptance_plot_filename": acceptance_plot_filename,
				"corner_plot_filename": corner_plot_filename,
				"pp_spectra_plot_filename": pp_spectra_plot_filename,
				"pp_observed_spectra_filenames": ",".join(pp_observed_spectra_filenames),
				"pp_modelled_spectra_filenames": ",".join(pp_modelled_spectra_filenames),
				"time_elapsed": int(time() - t_init),
				"final_mean_acceptance_fraction": mean_acceptance_fractions[-1],
				"sample_sigma_mean_acceptance_fraction": np.std(mean_acceptance_fractions[-model.configuration["solver"]["sample"]:])
			})

			# Update results with the posteriors
			logger.info("Posteriors:")
			for dimension, (posterior_value, pos_uncertainty, neg_uncertainty) in posteriors.iteritems():
				logger.info("\t{0:15s}: {1:.2f} (+{2:.2f}, -{3:.2f})".format(dimension, posterior_value,
					pos_uncertainty, neg_uncertainty))

				metadata.update({
					dimension: posterior_value,
					"u_maxabs_{0}".format(dimension): np.abs([neg_uncertainty, pos_uncertainty]).max(),
					"u_pos_{0}".format(dimension): pos_uncertainty,
					"u_neg_{0}".format(dimension): neg_uncertainty,
				})

			# Save the sampler chain
			blobs = np.array(sampler.blobs)
			blobs = blobs.reshape((-1, 1 + len(model.dimensions)))

			# Append an sample and step number
			walkers = model.configuration["solver"]["walkers"]
			chain = np.core.records.fromarrays(
				np.vstack([
					np.arange(1, 1 + len(blobs)),
					np.floor(np.arange(1, 1 + len(blobs)/walkers, 1.0/walkers)),
					blobs.T
				]),
				names=["Iteration", "Sample"] + model.dimensions + ["log(likelihood)"],
				formats=["i4", "i4"] + ["f8"] * (1 + len(model.dimensions)))

			# Save the chain
			primary_hdu = pyfits.PrimaryHDU()
			table_hdu = pyfits.BinTableHDU(chain)
			hdulist = pyfits.HDUList([primary_hdu, table_hdu])
			hdulist.writeto(chain_filename, clobber=True)

			try: metadata_columns
			except NameError:
				# Headers first
				metadata_columns = [] + header_columns

				# Dimensions (w/ uncertainties)
				for column in model.grid_points.dtype.names:
					metadata_columns.append(column)
					for prefix in ("u_maxabs_", "u_pos_", "u_neg_"):
						metadata_columns.append(prefix + column)

				# Nusiance parameters (w/ uncertainties)
				nusiance_columns = scope.utils.unique_preserved_list(list(model.grid_points.dtype.names) \
					+ model.dimensions)[len(model.grid_points.dtype.names):]
				for column in nusiance_columns:
					metadata_columns.append(column)
					for prefix in ("u_maxabs_", "u_pos_", "u_neg_"):
						metadata_columns.append(prefix + column)

				# Time elapsed, model
				metadata_columns.extend(["time_elapsed", "final_mean_acceptance_fraction", "sample_sigma_mean_acceptance_fraction", 
					"model", "run_id", "walkers", "burn", "sample", "input_filenames", "chain_filename", "state_filename", 
					"acceptance_plot_filename", "corner_plot_filename", "pp_spectra_plot_filename", "pp_observed_spectra_filenames",
					"pp_modelled_spectra_filenames"])
				metadata_formats = [("f8", "|S256")[isinstance(metadata[key], str)] for key in metadata_columns]

				all_results = np.core.records.fromrecords([[metadata[key] for key in metadata_columns]],
					names=metadata_columns, formats=metadata_formats)

			else:
				row = np.core.records.fromrecords([[metadata.get(key, np.nan) for key in metadata_columns]],
					dtype=all_results.dtype)
				all_results = np.append(all_results, row)

			# Write results to filename	
			primary_hdu = pyfits.PrimaryHDU()
			table_hdu = pyfits.BinTableHDU(all_results)
			hdulist = pyfits.HDUList([primary_hdu, table_hdu])
			hdulist.writeto(all_results_filename, clobber=True)

			# Close sampler pool
			if model.configuration["solver"].get("walkers", 1) > 1:
				sampler.pool.close()
				sampler.pool.join()

			# Save sampler state
			with open(state_filename, "wb") as fp:
				pickle.dump([sampler.chain[:, -1, :], sampler.lnprobability[:, -1], sampler.random_state], fp, -1)

			pp_parameters = dict(zip(posteriors.keys(), [posteriors[k][0] for k in posteriors.keys()]))
			
			pp_observed_spectra = model.observed_spectra(spectra, **pp_parameters)
			pp_modelled_spectra = model.model_spectra(observations=pp_observed_spectra, **pp_parameters)
			[spectrum.save(filename) for spectrum, filename in zip(pp_observed_spectra, pp_observed_spectra_filenames)]
			[spectrum.save(filename) for spectrum, filename in zip(pp_modelled_spectra, pp_modelled_spectra_filenames)]

			# Plot results
			if args.plotting:

				# Plot the mean acceptance fractions
				fig = plt.figure()
				ax = fig.add_subplot(111)
				ax.plot(mean_acceptance_fractions, "k", lw=2)
				ax.axvline(x=model.configuration["solver"]["burn"], c="#666666", zorder=-1, lw=1)
				ax.set_xlabel("Iterations")
				ax.set_ylabel("$\langle{}f_a\\rangle$")
				fig.savefig(acceptance_plot_filename)

				# Make a corner plot with just the parameters of interest
				indices = np.array([model.dimensions.index(dimension) for dimension in model.grid_points.dtype.names])
				fig = triangle.corner(blobs[-int(model.configuration["solver"]["sample"] * model.configuration["solver"]["walkers"]):, indices], labels=scope.utils.latexify(model.grid_points.dtype.names), extents=[0.99]*len(model.grid_points.dtype.names))
				fig.savefig(corner_plot_filename)

				# Plot spectra
				fig = plt.figure()
				for i, (aperture, modelled_aperture, observed_aperture) in enumerate(zip(model._mapped_apertures, pp_modelled_spectra, pp_observed_spectra)):

					ax = fig.add_subplot(len(spectra), 1, i+1)
					ax.plot(modelled_aperture.disp, modelled_aperture.flux, 'b')
					ax.plot(observed_aperture.disp, observed_aperture.flux, 'k')

					# Get the full boundaries
					if "masks" in model.configuration and aperture in model.configuration["masks"]:
						full_extent = \
							(np.min(model.configuration["masks"][aperture]), np.max(model.configuration["masks"][aperture]))

						ax.set_xlim(full_extent)

				fig.savefig(pp_spectra_plot_filename)

				# Closing the figures isn't enough; matplotlib leaks memory
				plt.close("all")

			# Delete some things
			del sampler, chain, primary_hdu, table_hdu, hdulist

	logger.info("Fin.")

if __name__ == "__main__":
	main()
